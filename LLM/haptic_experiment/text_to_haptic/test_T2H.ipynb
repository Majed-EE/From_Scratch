{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469d8c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# ######################################\n",
    "# Utilities: dataset & vocab\n",
    "\n",
    "class SimpleVocab:\n",
    "    def __init__(self, min_freq=0, reserved_tokens=None):\n",
    "        self.min_freq = min_freq\n",
    "        self.freqs = Counter() # frequency counter for tokens\n",
    "        self.token_to_idx = {}\n",
    "        self.idx_to_token = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = [\"<pad>\", \"<unk>\", \"<cls>\"]\n",
    "        for t in reserved_tokens:\n",
    "            self.add_token(t)\n",
    "        self.reserved_tokens = reserved_tokens\n",
    "\n",
    "    def add_token(self, t): # Dynamic token adder\n",
    "        if t in self.token_to_idx:\n",
    "            return # already added , return nothing?\n",
    "        idx = len(self.idx_to_token) # if new token\n",
    "        self.token_to_idx[t] = idx # token to idx is dict\n",
    "        self.idx_to_token.append(t) # list, although why do we need a list ``\\../``\n",
    "\n",
    "    def add_sentence(self, sent_tokens):\n",
    "        self.freqs.update(sent_tokens)\n",
    "\n",
    "    def build_vocab(self):\n",
    "        # add tokens meeting min_freq\n",
    "        for token, freq in self.freqs.most_common():\n",
    "            if freq >= self.min_freq and token not in self.token_to_idx:\n",
    "                self.add_token(token)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def encode(self, tokens):\n",
    "        unk = self.token_to_idx.get(\"<unk>\")\n",
    "        return [self.token_to_idx.get(t, unk) for t in tokens] # return token ids, if token unknown then return unknown, get does is if t not in token_to_idx then return unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdd11c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simple_tokenize(text):\n",
    "    # basic lowercase whitespace tokenizer \n",
    "    text = text.lower()\n",
    "   \n",
    "    for ch in [\",\", \".\", \";\", \":\", \"!\", \"?\", \"(\", \")\", \"\\\"\", \"'\"]:\n",
    "        text = text.replace(ch, \" \")\n",
    "    tokens = text.strip().split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aea86bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'this', 'is', 'a', 'test']\n"
     ]
    }
   ],
   "source": [
    "sample_text=simple_tokenize(\"Hello, world! This is a test.\")\n",
    "print(sample_text)\n",
    "\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"new_shape_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fcb453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this marble is giant and stiff', 'a soft round object was miniature', 'bulky and flexible describes the tube', 'the canister was solid and large']\n",
      "['hello', 'world', 'this', 'is', 'a', 'test']\n"
     ]
    }
   ],
   "source": [
    "sample_text=simple_tokenize(\"Hello, world! This is a test.\")\n",
    "k=df[\"sentence\"][:4]\n",
    "k=list(k)\n",
    "sent=\"\"\n",
    "for i in k:\n",
    "    sent=simple_tokenize(i)\n",
    "# k.rows = list(zip(df[\"sentence\"].str.strip(), df[\"type\"], df[\"size\"], df[\"stiffness\"]))\n",
    "\n",
    "\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2bba35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> size stiffness\n",
      "sentence\n"
     ]
    }
   ],
   "source": [
    "sentence,shape, size, stiff = df.columns[0], df.columns[1], df.columns[2], df.columns[3]\n",
    "\n",
    "print(type(size), size, stiff)\n",
    "print(df.columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bb96c76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example=list(df.loc[:5,\"sentence\"])\n",
    "# print(example)\n",
    "# vocab=SimpleVocab()\n",
    "# for sent in example:\n",
    "#     tokens = simple_tokenize(sent)\n",
    "#     vocab.add_sentence(tokens)\n",
    "#     vocab.add_sentence(sent)\n",
    "# vocab.build_vocab()\n",
    "# toks=example[0]\n",
    "# j=vocab.encode(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44836b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "# print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "911b6e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this marble is giant and stiff', 'a soft round object was miniature', 'bulky and flexible describes the tube', 'the canister was solid and large']\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=df[\"sentence\"][:4].str.strip() # This line selects the first four rows of the \"sentence\" column, removes any leading or trailing whitespace, and assigns the result to the variable k.\n",
    "# print(k)\n",
    "\n",
    "k=[i for i in k]\n",
    "print(k)\n",
    "print(len(k))\n",
    "type(df[\"type\"][:4].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d442888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiTaskTextDataset(Dataset):\n",
    "    def __init__(self, csv_path, vocab=None, max_len=10, build_vocab=True): # max len is the contenxt size\n",
    "        # read CSV\n",
    "        self.df=pd.read_csv(csv_path)\n",
    "        self.row,self.shape,self.size,self.stiff= self.df[\"sentence\"],self.df[\"type\"], self.df[\"size\"], self.df[\"stiffness\"]\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.vocab = vocab\n",
    "        self.data=[]\n",
    "   \n",
    "        # build vocab if requested\n",
    "        if vocab is None and build_vocab:\n",
    "            self.vocab = SimpleVocab(min_freq=1)\n",
    "            for text in self.row:\n",
    "                toks = simple_tokenize(text)\n",
    "                self.vocab.add_sentence(toks)\n",
    "            self.vocab.build_vocab()\n",
    "        elif vocab is None:\n",
    "            raise ValueError(\"Provide vocab or set build_vocab=True\")\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "\n",
    "        # tokenize-> encode-> append values\n",
    "        # self.data = []\n",
    "        for text, shape, size, stiff in zip(self.row, self.shape, self.size, self.stiff):\n",
    "            toks = simple_tokenize(text)\n",
    "            enc = self.vocab.encode(toks)\n",
    "            # Add CLS at start\n",
    "            cls_idx = self.vocab.token_to_idx[\"<cls>\"]\n",
    "            enc = [cls_idx] + enc\n",
    "            # pad/truncate\n",
    "            if len(enc) < self.max_len:\n",
    "                enc = enc + [self.vocab.token_to_idx[\"<pad>\"]] * (self.max_len - len(enc))\n",
    "            else:\n",
    "                enc = enc[: self.max_len]\n",
    "            # print(f\"Encoded: {enc} shape: {shape} size: {size} stiffness: {stiff} \")\n",
    "            try:\n",
    "                self.data.append((\n",
    "                    torch.tensor(enc, dtype=torch.long),\n",
    "                    torch.tensor(shape, dtype=torch.long),\n",
    "                    torch.tensor(size, dtype=torch.long),\n",
    "                    torch.tensor(stiff, dtype=torch.long),\n",
    "                ))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "370e5432",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"new_shape_dataset.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path = \"new_shape_dataset.csv\"\n",
    "multi_dataset=MultiTaskTextDataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c4d1b008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "j=multi_dataset.get_data()[0]\n",
    "print(type(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9cecabbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([4, 1])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4])\n",
    "print(x.shape)\n",
    "y=torch.unsqueeze(x, 0)\n",
    "print(y.shape)\n",
    "z=torch.unsqueeze(x, 1)\n",
    "print(z.shape)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330d07ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Model: PosEncoding\n",
    "# -------------------------\n",
    "class PositionalEncoding(nn.Module): \n",
    "    def __init__(self, d_model, max_len=512): # max_len is what exactly -> context size?\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()  # (max_len,1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer(\"pe\", pe) # what does register buffer do\n",
    "        # used to register a buffer that should not be considered a model parameter\n",
    "        # persistant is across batches meaning, non persistant means it wont be a part of state_dict \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e92138c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VanillaTransformerMultiTask(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, hidden_dim,\n",
    "                 num_shape, num_size, num_stiff, max_len=32, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=0) # what is padding_idx?\n",
    "        # This module is often used to store word embeddings and retrieve them using indices. \n",
    "        # The input to the module is a list of indices, and the output is the corresponding word embeddings.\n",
    "        # padding_idx do not contribute to the gradient; therefore, the embedding vector at padding_idx is not updated during training, i\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len=max_len)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                                   dim_feedforward=hidden_dim,\n",
    "                                                   dropout=dropout, activation=\"relu\")\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        # pooling: use token 0 (we placed <cls> at index 0) as pooled\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # output heads\n",
    "        self.head_shape = nn.Linear(d_model, num_shape)\n",
    "        self.head_size = nn.Linear(d_model, num_size)\n",
    "        self.head_stiff = nn.Linear(d_model, num_stiff)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "\n",
    "        #  nn.init.xavier_uniform_ sets the weights so that the variance is the same across layers, which helps with stable training.       \n",
    "        # how stable? i dont know\n",
    "        nn.init.xavier_uniform_(self.token_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.dense.weight)\n",
    "        nn.init.xavier_uniform_(self.head_shape.weight)\n",
    "        nn.init.xavier_uniform_(self.head_size.weight)\n",
    "        nn.init.xavier_uniform_(self.head_stiff.weight)\n",
    "\n",
    "    def forward(self, input_ids, src_key_padding_mask=None):\n",
    "        # what is src_key_padding?\n",
    "        # input_ids: (batch, seq_len)\n",
    "        x = self.token_emb(input_ids)  # (batch, seq_len, d_model)\n",
    "        x = self.pos_enc(x)  # x: (batch, seq_len, d_model)\n",
    "        # transformer expects (seq_len, batch, d_model)\n",
    "        x = x.transpose(0, 1)\n",
    "        # src_key_padding_mask: (batch, seq_len) boolean: True for padded positions\n",
    "        enc = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)  # (seq_len, batch, d_model)\n",
    "        enc = enc.transpose(0, 1)  # (batch, seq_len, d_model)\n",
    "        cls_token = enc[:, 0, :]  # (batch, d_model)\n",
    "        pooled = torch.tanh(self.dense(cls_token))\n",
    "        pooled = self.dropout(pooled)\n",
    "        out_w = self.head_wealth(pooled)\n",
    "        out_b = self.head_body(pooled)\n",
    "        out_g = self.head_gender(pooled)\n",
    "        return out_w, out_b, out_g\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Training & Eval helpers\n",
    "# -------------------------\n",
    "\n",
    "# what does it do \n",
    "def collate_batch(batch):\n",
    "    # batch: list of tuples: (enc, w, b, g)\n",
    "    encs = torch.stack([b[0] for b in batch], dim=0)\n",
    "    w = torch.stack([b[1] for b in batch], dim=0)\n",
    "    bod = torch.stack([b[2] for b in batch], dim=0)\n",
    "    g = torch.stack([b[3] for b in batch], dim=0)\n",
    "    # padding mask: True where pad token (token idx 0) exists\n",
    "    pad_mask = encs == 0\n",
    "    return encs, pad_mask, w, bod, g\n",
    "\n",
    "\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    return (preds.argmax(dim=1) == labels).float().mean().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4927d35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(model, dataloader, optim, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_acc_w = 0.0\n",
    "    total_acc_b = 0.0\n",
    "    total_acc_g = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for encs, pad_mask, w, bod, g in dataloader:\n",
    "        encs = encs.to(device)\n",
    "        pad_mask = pad_mask.to(device)\n",
    "        w = w.to(device)\n",
    "        bod = bod.to(device)\n",
    "        g = g.to(device)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        out_w, out_b, out_g = model(encs, src_key_padding_mask=pad_mask)\n",
    "        loss_w = criterion(out_w, w)\n",
    "        loss_b = criterion(out_b, bod)\n",
    "        loss_g = criterion(out_g, g)\n",
    "        loss = loss_w + loss_b + loss_g\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_loss += loss.item() * encs.size(0)\n",
    "        total_acc_w += accuracy(out_w.detach().cpu(), w.detach().cpu()) * encs.size(0)\n",
    "        total_acc_b += accuracy(out_b.detach().cpu(), bod.detach().cpu()) * encs.size(0)\n",
    "        total_acc_g += accuracy(out_g.detach().cpu(), g.detach().cpu()) * encs.size(0)\n",
    "\n",
    "    n = len(dataloader.dataset)\n",
    "    return total_loss / n, total_acc_w / n, total_acc_b / n, total_acc_g / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac2dacc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58da12a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ef3a63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da1288c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5024a348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
