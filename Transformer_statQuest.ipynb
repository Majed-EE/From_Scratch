{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8932d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f58bf898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step one\n",
    "# word embedddings\n",
    "\n",
    "text=\"Lets to go\"\n",
    "\n",
    "# super simple vocab lets, to, go as 1 2 3 for simplicity, it is from scratch after all  \n",
    "vocab=text.split(\" \")\n",
    "vocab=vocab+[\"<EOS>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c6b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b00cfa0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Lets': 0, 'to': 1, 'go': 2, '<EOS>': 3}\n",
      "torch.Size([1, 3])\n",
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "WtoVec={key:i for i,key in enumerate(vocab)}\n",
    "print(WtoVec)\n",
    "# input connected to activation functions \n",
    "# encode \n",
    "encode = lambda c: WtoVec[c] #if type(c)==str # encoder: take a string, output a list of integers \n",
    "def vect(k):\n",
    "    b=torch.zeros(1,4)\n",
    "    b[0][k]=1\n",
    "    return b\n",
    "\n",
    "WtoVec[\"Lets\"]\n",
    "# decode \n",
    "\n",
    "ex1=torch.tensor([0,1,3]).reshape(1,3) # lets go \n",
    "ex2=torch.tensor([0,1,3]).reshape(1,3) # To go\n",
    "vocab_size=len(vocab)\n",
    "n_embd=4\n",
    "\n",
    "# C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "\n",
    "\n",
    "print(ex1.shape)\n",
    "\n",
    "\n",
    "\n",
    "weights_1=torch.randn(4,1)\n",
    "print(weights_1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c3543e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "tensor([ 0.7055, -1.4745])\n"
     ]
    }
   ],
   "source": [
    "input_vec=torch.zeros(4,4)\n",
    "for i,key in enumerate(WtoVec):\n",
    "    input_vec[i]=vect(WtoVec[key])\n",
    "print(input_vec)\n",
    "C=torch.randn(4,4)\n",
    "key=\"Lets\"\n",
    "embd=C[torch.tensor(WtoVec[key])]\n",
    "print(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aac80a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two activation function\n",
    "\n",
    "def activation1(vect):\n",
    "    return torch.sum(vect,dim=1,keepdim=True)\n",
    "def activation2(vect):\n",
    "    return torch.sum(vect,dim=1,keepdim=True)\n",
    "\n",
    "    \n",
    "# word embedding done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617bc7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my house is beautiful "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932500a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d0fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# position encoding help the transformer to track order\n",
    "position_encoding= lambda pos,word: np.cos(2 * np.pi * pos * word+0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3538f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the word and positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a916600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self attention-> correctly accociate word with meaning\n",
    "# self attention calculates the similarity between the first word, and all the words in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7d5a240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# muptiply the values the resultant numbers to get two new numbers- querry\n",
    "# create keys for all the words\n",
    "# use query and keys to calculate the similarity between itself(querry) and other words(keys).\n",
    "# similarity between querry and keys is dot product\n",
    "# Querry dot key is the similarity\n",
    "# we want to have the word which is more influence to have more say in the encoding of the the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5d68f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to properly encode lets we will create two more values called values- directly multiply it with the position+source embeding\n",
    "# we scale the vlaue lets by 1.0 and create value for go and scale it by 0? \n",
    "# add the scaled value together-> combine separate encodings for both input words, lets and go to create self attention values for lets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d54a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights we use to create self attention values for lets and go are the same \n",
    "# we also use same weight for all words to calculate self attention keys and value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e68e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we calculate the queries keys and values for each word at the same time.\n",
    "# the new self attention values for each word contain input from all of the other words, \n",
    "# and this heps give each word context- helps us to tell how each word is related to other\n",
    "\n",
    "# if we think of this unit with it's weights for calculating Queries, Keys and Values, as self- Attention cell\n",
    "# we can create a stach of self-attention cells., each with it's own sets of weights. that we appply to the poistion encoded values for each word, \n",
    "# to caputre different relationships among the words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e01458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to encode we take position encoded values and add them to self attention values- bypass is called residual connections and they make \n",
    "# it easier to train complex NN\n",
    "# this allws self-attention layer to establish relationships amoung the input words without having to also preserve the word embedding and\n",
    "# position encoding information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052cce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of attention encoder-\n",
    "# -word embedding, positional encoding, self attention and residual connections\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21514be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb77add",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# start with <eos> usually or <sos>\n",
    "# plug in and create embeddings \n",
    "# add positional encodings\n",
    "# we create querry keys and values for each token and calculate self attention just like before\n",
    "# the sets of weights we used to calculate the decoders self attnetion query keys and values are different \n",
    "# from the sets we used in the Encoder.\n",
    "# consolidate the maths and create residual blocks\n",
    "# while decoding we also need to keep track of the relationshiop bweten the input sentence and the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b67da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we create queries in the decoder and find similartiy with each of the keys in the encoder and run the softmax function\n",
    "# the sets of Weights that we use to calculate the Queries, keys and Valuse for encoder, Decoder Attention are different from \n",
    "# the sets of weights we use for selfAttnetion.\n",
    "# create residual connections\n",
    "# we run the final 2 values (after query key and values upgrade and adding residual connections) \n",
    "# we pass it to fully connected neural network \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45379991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440f5f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence to sequence encoder decoder --> long short term memory and wrod to vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3088ce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary contains a mix of words and sumbols we refer to the individual elemets in a vocabulary as tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a209d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weigths and biases in the LSTM cells and embedding layer is the same for all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9e49ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last long and short term memories (the cell and hidden states) from both layers of the LSTM cells in the \n",
    "# Encoder are called the context vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3e650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decoder is lstm like the encoder such that context vector of encoder is passed \n",
    "# Through the input of the decoder starting from <EOS> \n",
    "# Usually output values of the top layer of lstm cells is passed through fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7e75bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attnetion in neural network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention is all you neeed paper\n",
    "# approaches in sequence nodeling a nd transduction problems such as language modeling and machine translation "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
